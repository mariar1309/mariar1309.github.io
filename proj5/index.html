<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 5 - Fun with Diffusion Models!</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Sidebar Navigation -->
    <nav class="sidebar">
        <div class="sidebar-content">
            <h3>Contents</h3>
            <ul>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#part0">Part A: Diffusion Models</a>
                    <ul>
                        <li><a href="#part01">Part 0: Setup</a></li>
                        <li><a href="#part02">Part 1: Sampling Loops</a></li>
                        <li><a href="#part11">Part 1.1: Forward Process</a></li>
                        <li><a href="#part12">Part 1.2: Classical Denoising</a></li>
                        <li><a href="#part13">Part 1.3: One-Step Denoising</a></li>
                        <li><a href="#part14">Part 1.4: Iterative Denoising</a></li>
                        <li><a href="#part15">Part 1.5: Diffusion Sampling</a></li>
                        <li><a href="#part16">Part 1.6: CFG</a></li>
                        <li><a href="#part17">Part 1.7: Image-to-Image</a></li>
                        <li><a href="#part171">Part 1.7.1: Hand-Drawn Images</a></li>
                        <li><a href="#part172">Part 1.7.2: Inpainting</a></li>
                        <li><a href="#part173">Part 1.7.3: Text-Conditional</a></li>
                        <li><a href="#part18">Part 1.8: Visual Anagrams</a></li>
                        <li><a href="#part19">Part 1.9: Hybrid Images</a></li>
                    </ul>
                </li>
                <li><a href="#partb">Part B: Flow Matching</a></li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <div class="main-wrapper">
        <div class="container">
            <header class="header">
                <h1>Project 5: Fun with Diffusion Models!</h1>
                <h4>CS 180 Fall 2025 – Maria Rufova</h4>
            </header>

        <!-- Introduction -->
        <section class="introduction" id="introduction">
            <h2>Introduction</h2>
            <p>
                In this project I implemented and deployed diffusion models for image generation.  
                In part A, I worked with the DeepFloyd IF model.
                Diffusion models work by gradually adding noise to images and then learning to reverse this process, enabling them to 
                generate high-quality images from pure noise or perform various image editing tasks. In this part I implemented 
                the core sampling loops and experimented with Classifier-Free Guidance (CFG) to docreative tasks such as
                inpainting, visual anagrams, and hybrid images.
                In part B I will implement Flow Matching from scratch. 
            </p>
        </section>

        <!-- Part 0 -->
        <section class="subsection" id="part0">
            <h1>Part A: The Power of Diffusion Models</h1>
        </section>

        <section class="partA1" id="part01">
            <h2>Part 0: Setup</h2>
            <p>
                Throughout part A I used the DeepFloyd IF diffusion model, a two-stage text-to-image model that produces 64×64 images. 
                To use the model, I generated prompt embeddings using the provided Hugging Face clusters, creating a dictionary of embeddings for various 
                creative text prompts. Some example prompts and their corresponding images are shown below. 
            </p>
            <p>
                I experimented with different numbers of <code>num_inference_steps</code> (20, 40, 60, 80, 100) to observe how this parameter affects image quality. 
                More inference steps generally produce higher quality images but take longer to generate.
                In the examples below, 100 inference steps produced final images that produced finer details and more closely replicated the prompted art style than
                when those same prompts ran with 20 steps. 
                For all subsequent parts of this project, 
                I used a random seed of <strong>100</strong> to make sure I can reproduce my results later, and kept this same seen throughout the rest of part A. 
            </p>
            <p>
                Three examples of images generated with my custom prompts (20 vs 100 inference steps):
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/ada_love_2.jpeg" alt="Ada Lovelace" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 20) 'A painting of Ada Lovelace in Alphonse Mucha's style'</figcaption>
                </figure>   
                <figure>
                    <img src="web_images/fuji_3.jpeg" alt="Mount Fuji" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 20) 'A painting of Mount Fuji in Claude Monet's style'</figcaption>
                </figure> 
                <figure>
                    <img src="web_images/ferrari_1.jpeg" alt="Ferrari" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 20) 'A painitng of red Ferrari car in Kazimir Malevich style'</figcaption>
                </figure>      
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/ada_100.jpeg" alt="Ada Lovelace" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 100) 'A painting of Ada Lovelace in Alphonse Mucha's style'</figcaption>
                </figure>   
                <figure>
                    <img src="web_images/fuji_100.jpeg" alt="Mount Fuji" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 100) 'A painting of Mount Fuji in Claude Monet's style'</figcaption>
                </figure> 
                <figure>
                    <img src="web_images/ferrari_100.jpeg" alt="Ferrari" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 100) 'A painitng of red Ferrari car in Kazimir Malevich style'</figcaption>
                </figure>      
            </div>
        </section>

        <section class="partA2" id="part02">
            <h2>Part 1: Sampling Loops</h2>
            <p>
                This section introduces the core concepts of diffusion models. A diffusion model works in two phases: the forward process 
                gradually adds Gaussian noise to a clean image over T timesteps (T=1000 for DeepFloyd), while the reverse process learns 
                to denoise and recover the original image. By starting from pure noise and iteratively denoising, we can generate new images 
                from scratch.
            </p>
            <p>
                The forward process is defined by: <strong>x_t = √(ᾱ_t) · x_0 + √(1 - ᾱ_t) · ε</strong>, where x_0 is the clean image, 
                x_t is the noisy image at timestep t, and ε is Gaussian noise. The noise schedule coefficients ᾱ_t control how much noise 
                is added at each step.
            </p>
            <p>
                In the following sections, I implement these sampling loops from scratch and explore their various applications.
            </p>
        </section>

        <section class="partA3" id="part11">
            <h2>Part 1.1: Implementing the Forward Process</h2>
            <p>
                The forward process is the foundation of diffusion models. I implemented a function that takes a clean image and adds 
                a specific amount of Gaussian noise based on the timestep t. The formula combines the scaled original image with scaled 
                random noise: <strong>x_t = √(ᾱ_t) · x_0 + √(1 - ᾱ_t) · ε</strong>.
            </p>
            <p>
                Using the alphas_cumprod schedule (where ᾱ_t ≈ 1 for small t and ᾱ_t ≈ 0 for large t), I applied this process to 
                the Campanile test image at three different noise levels: t=250 (slight noise), t=500 (moderate noise), and t=750 
                (heavy noise). As expected, the image becomes progressively more corrupted as t increases, eventually resembling pure 
                Gaussian noise at high timestep values.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_1.jpeg" alt="Forward process on Campanile">
                    <figcaption>The Campanile at Different Noise Levels (t=0, 250, 500, 750)</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA4" id="part12">
            <h2>Part 1.2: Classical Denoising</h2>
            <p>
                To establish a baseline, I attempted to denoise the noisy Campanile images using classical Gaussian blur filtering. 
                This method simply applies spatial smoothing to reduce noise, without any understanding of the underlying image structure.
            </p>
            <p>
                I experimented with different kernel sizes (3×3 for t=250, 5×5 for t=500, 7×7 for t=750) to find the best denoising 
                for each noise level. As shown below, Gaussian blur performs poorly – it removes noise but also destroys fine details 
                and creates a blurry result. For heavily noised images (t=750), the method essentially fails to recover any meaningful 
                structure. This demonstrates why we need more sophisticated learned approaches like diffusion models.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_2.jpeg" alt="Classical denoising comparison">
                    <figcaption>Top: Noisy images at t=250, 500, 750. Bottom: Gaussian blur denoised versions</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA5" id="part13">
            <h2>Part 1.3: One-Step Denoising</h2>
            <p>
                Now I used the pretrained DeepFloyd UNet to denoise images in a single step. The UNet was trained on millions of 
                noisy/clean image pairs and learned to predict the noise component in a noisy image. Given x_t and timestep t, 
                the UNet outputs an estimate of the noise ε, which I can use to recover the clean image: 
                <strong>x_0 = (x_t - √(1 - ᾱ_t) · ε) / √(ᾱ_t)</strong>.
            </p>
            <p>
                The UNet is conditioned on text prompts (I used "a high quality photo") and outputs 6 channels – the first 3 are 
                the noise estimate, and the last 3 are variance estimates. Compared to Gaussian blur, one-step denoising produces 
                dramatically better results, successfully recovering recognizable structure even from heavily noised images. However, 
                quality degrades at higher noise levels (t=750) since estimating the clean image from pure noise in one step is 
                extremely difficult.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_3.jpeg" alt="One-step denoising results">
                    <figcaption>Top: Noisy images. Bottom: One-step denoised results at t=250, 500, 750</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA6" id="part14">
            <h2>Part 1.4: Iterative Denoising</h2>
            <p>
                To improve quality, I implemented iterative denoising, which gradually removes noise over multiple steps. Instead of 
                running all 1000 timesteps, I used a strided schedule (990, 960, 930, ..., 30, 0) with steps of 30 for efficiency.
            </p>
            <p>
                At each step, I: (1) predict x_0 using the UNet, (2) compute the next slightly-less-noisy image x_t' using the DDPM 
                formula that interpolates between the predicted clean image and directional noise, and (3) add a small amount of 
                predicted variance. The key formula is: 
                <strong>x_t' = √(ᾱ_t') · β_t / (1 - ᾱ_t) · x_0 + √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t) · x_t + variance</strong>.
            </p>
            <p>
                Starting from i_start=10 (t=690), I show the denoising progression every 5 iterations. The image gradually becomes 
                clearer, demonstrating how iterative refinement produces much better results than one-step denoising. The final comparison 
                shows iterative denoising produces the sharpest, most detailed reconstruction compared to one-step denoising and Gaussian blur.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_4_a.jpeg" alt="Iterative denoising progression">
                    <figcaption>Denoising progression at t=90, 240, 390, 540, 690 (less noise → more noise)</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_4_b.jpeg" alt="Denoising methods comparison">
                    <figcaption>Comparison: Original, Iteratively Denoised, One-Step Denoised, Gaussian Blur</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA7" id="part15">
            <h2>Part 1.5: Diffusion Model Sampling</h2>
            <p>
                With iterative denoising working, I can now generate images from scratch! By starting with pure random noise (x_T) 
                and running iterative denoising from i_start=0, the model "hallucinates" entirely new images that match the prompt.
            </p>
            <p>
                I generated 5 samples using the prompt "a high quality photo". The results are coherent but somewhat generic and 
                lack fine details. This is expected without Classifier-Free Guidance – the model produces diverse samples but they're 
                not strongly tied to the prompt. The quality will improve significantly in the next section with CFG.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_5_b.jpeg" alt="Diffusion model samples">
                    <figcaption>5 images sampled from pure noise with prompt "a high quality photo"</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA8" id="part16">
            <h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>
            <p>
                Classifier-Free Guidance dramatically improves image quality by amplifying the influence of the text prompt. The technique 
                computes both conditional (ε_cond with prompt) and unconditional (ε_uncond with empty prompt "") noise estimates, then 
                extrapolates away from the unconditional estimate: <strong>ε = ε_uncond + scale · (ε_cond - ε_uncond)</strong>.
            </p>
            <p>
                With scale=7, the model produces much sharper, more coherent images that strongly adhere to the prompt. The tradeoff is 
                reduced diversity – higher CFG values sacrifice variety for quality. Comparing these 5 CFG samples to the non-CFG samples 
                from Part 1.5, the improvement is striking: images are more detailed, better composed, and look more "real".
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_6_b.jpeg" alt="CFG samples">
                    <figcaption>5 images with CFG (scale=7) – much higher quality than without CFG</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA9" id="part17">
            <h2>Part 1.7: Image-to-Image Translation</h2>
            <p>
                Using the SDEdit algorithm, I can edit existing images by adding noise and then denoising with CFG. The amount of noise 
                (controlled by i_start) determines how much the image changes: low i_start (little noise) preserves the original structure, 
                while high i_start (more noise) allows more creative freedom.
            </p>
            <p>
                I tested this with i_start values [1, 3, 5, 7, 10, 20] on three images: the Campanile, a horse photo, and a photo of my 
                friend Basil. With i_start=1, the edited image is nearly identical to the original. As i_start increases, the model makes 
                more significant edits while maintaining the overall composition. At i_start=20, the images are quite different but still 
                recognizable as the same subject. This demonstrates how diffusion models can "project" images onto the manifold of natural images.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_7_a.jpeg" alt="Campanile SDEdit">
                    <figcaption>Campanile edits at noise levels 1, 3, 5, 7, 10, 20, plus original</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_7_b.jpeg" alt="Horse SDEdit">
                    <figcaption>Horse image edits at different noise levels with "a high quality photo"</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_b.jpeg" alt="Basil SDEdit">
                    <figcaption>Basil image edits showing gradual transformation</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA10" id="part171">
            <h2>Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
            <p>
                This technique works particularly well on non-realistic inputs like sketches and drawings. I tested it on three images: 
                a web sketch of Berkeley, and my hand-drawn sheep and tomato. The diffusion model "projects" these simple drawings onto 
                the natural image manifold, adding realistic textures, lighting, and details while preserving the original composition.
            </p>
            <p>
                The results are remarkable – simple line drawings are transformed into photorealistic images. The Berkeley sketch becomes 
                a convincing architectural photo, the sheep sketch gains fluffy wool texture and realistic shading, and the tomato drawing 
                acquires the glossy appearance of a real tomato. This demonstrates the model's ability to understand and extrapolate from 
                minimal visual information.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/berk_sketch.jpeg" alt="Berkeley sketch" style="max-width: 200px; height: auto;">
                </figure>
                <figure>
                    <img src="web_images/sheep_sketch.jpeg" alt="Sheep sketch">
                    <figcaption>Original sketches and drawings</figcaption>
                </figure>
                <figure>
                    <img src="web_images/tomato_sketch.jpeg" alt="Tomato sketch">
                </figure>
            </div>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_1_web.jpeg" alt="Berkeley processed">
                    <figcaption>Berkeley sketch transformed at different noise levels</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/sheep_process.jpeg" alt="Sheep processed">
                    <figcaption>My sheep drawing becoming photorealistic</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/tomato_progress.jpeg" alt="Tomato processed">
                    <figcaption>My tomato drawing gaining realistic texture and lighting</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA11" id="part172">
            <h2>Part 1.7.2: Inpainting</h2>
            <p>
                Following the RePaint paper, I implemented inpainting to fill masked regions with new content. The algorithm runs the 
                normal denoising loop, but at each step, replaces the unmasked regions with the properly-noised original image: 
                <strong>x_t ← mask · x_t + (1 - mask) · forward(x_orig, t)</strong>.
            </p>
            <p>
                This forces the model to preserve the original image outside the mask while generating plausible content inside the mask 
                that blends naturally with its surroundings. I demonstrate this by inpainting the top of the Campanile (making the model 
                hallucinate a new tower top), removing parts of the horse image, and editing my friend Basil's photo. The model generates 
                convincing fills that maintain consistency with the surrounding image context.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_7_1_c.jpeg" alt="Campanile inpaint">
                    <figcaption>Campanile: Original, Mask, Region to Replace, Inpainted Result</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_2_horse.jpeg" alt="Horse inpaint">
                    <figcaption>Horse inpainting result</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_2_basil.jpeg" alt="Basil inpaint">
                    <figcaption>Basil inpainting result</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA12" id="part173">
            <h2>Part 1.7.3: Text-Conditional Image-to-Image Translation</h2>
            <p>
                Building on SDEdit, I now guide the image projection with specific text prompts instead of the generic "a high quality photo". 
                This provides semantic control over the editing process – the model tries to make the image look like the text description 
                while preserving its structure.
            </p>
            <p>
                I experimented with prompts like "a marble statue", "a rocket ship", "sourdough bread dough rising", and "a colorful garden 
                with flowers". At low noise levels (i_start=1-3), the transformations are subtle. At higher noise levels (i_start=10-20), 
                the subjects take on clear characteristics of the prompt – the Campanile becomes marble-textured, the horse looks bread-like, 
                and Basil's photo gains floral colors and patterns. This demonstrates impressive prompt-guided style transfer while maintaining 
                compositional structure.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_rocket.jpeg" alt="Rocket Campanile">
                    <figcaption>'A rocket ship' transformation of Campanile</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_a.jpeg" alt="Marble Campanile">
                    <figcaption>'A marble statue' transformation of Campanile</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_e.jpeg" alt="Bread horse">
                    <figcaption>'Sourdough bread dough rising' transformation of horse</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_c.jpeg" alt="Garden Basil">
                    <figcaption>'A colorful garden with flowers' transformation of Basil</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA13" id="part18">
            <h2>Part 1.8: Visual Anagrams</h2>
            <p>
                Visual anagrams are optical illusions where an image shows one thing normally but reveals something different when flipped 
                upside down. I implemented this by computing noise estimates for both orientations and averaging them: 
                <strong>ε = (ε_1 + flip(ε_2)) / 2</strong>, where ε_1 comes from denoising the normal image with prompt p_1, and ε_2 comes 
                from denoising the flipped image with prompt p_2.
            </p>
            <p>
                I created three visual anagrams: (1) Count Dracula / Gothic Cathedral – the flowing cape transforms into cathedral spires 
                when flipped, (2) Old Man / Natural Landscape – facial features become landscape elements, and (3) Grand Piano / Staircase – 
                the piano keys and body structure naturally invert to form ascending stairs. These illusions required multiple attempts to 
                generate good results, as the two prompts must have compatible shapes. The results are striking examples of how diffusion 
                models can satisfy two contradictory visual interpretations simultaneously.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_8_drac.jpeg" alt="Dracula anagram">
                    <figcaption>Count Dracula / Gothic Cathedral visual anagram</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_8_man_b.jpeg" alt="Old man anagram">
                    <figcaption>Oil painting of old man / Natural landscape visual anagram</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_8_piano_b.jpg" alt="Piano view" style="max-width: 500px; height: auto;">
                    <figcaption>A grand piano viewed from above</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_8_stairs_b.jpg" alt="Stairs view" style="max-width: 500px; height: auto;">
                    <figcaption>A staircase going up (same image flipped)</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA14" id="part19">
            <h2>Part 1.9: Hybrid Images</h2>
            <p>
                Following the Factorized Diffusion paper, I created hybrid images that show different content when viewed up close versus 
                far away. The technique combines low frequencies from one noise estimate with high frequencies from another: 
                <strong>ε = f_lowpass(ε_1) + f_highpass(ε_2)</strong>, where I used Gaussian blur (kernel=33, sigma=2) for filtering.
            </p>
            <p>
                I created four hybrid images: (1) Skull / Waterfall – up close you see skull details, from far away it looks like a waterfall, 
                (2) Sheet Music / Birds on Wire – music notes up close, bird silhouettes from afar, (3) Artist's Palette / Colorful Garden – 
                paint blobs versus flowers, and (4) Swirled Frosting / Rose – frosting texture versus rose shape. These work because the prompt 
                pairs share similar structures at different scales: linear patterns (notes/birds), color distributions (palette/garden), and 
                spiral patterns (frosting/rose). The results demonstrate how our visual system processes different spatial frequencies at 
                different viewing distances.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_9_skull.jpg" alt="Skull hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid: Skull (close) / Waterfall (far)</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_9_birds_c.jpeg" alt="Birds hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid: Sheet music (close) / Birds on wire (far)</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_9_palette.jpg" alt="Palette hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid: Colorful garden (close) / Artist's palette (far)</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_9_rose.jpeg" alt="Rose hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid: Swirled frosting (close) / Rose flower (far)</figcaption>
                </figure>
            </div>
        </section>

        <section class="subsection" id="partb">
            <h1>Part B: Flow Matching From Scratch</h1>
            <p>Come back here in a week – Part B implementation in progress!</p>
        </section>

    </div>
    <script>
        window.addEventListener('scroll', function() {
            let sections = document.querySelectorAll('section[id]');
            let scrollPosition = window.scrollY + 100;
            sections.forEach(section => {
                let sectionTop = section.offsetTop;
                let sectionHeight = section.offsetHeight;
                let sectionId = section.getAttribute('id');
                
                if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
                    document.querySelectorAll('.sidebar a').forEach(link => {
                        link.classList.remove('active');
                    });
                    let activeLink = document.querySelector(`.sidebar a[href="#${sectionId}"]`);
                    if (activeLink) {
                        activeLink.classList.add('active');
                    }
                }
            });
        });
    </script>
</body>
</html>