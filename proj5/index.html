<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 5 - Fun with Diffusion Models!</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <!-- Sidebar Navigation -->
    <nav class="sidebar">
        <div class="sidebar-content">
            <h3>Contents</h3>
            <ul>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#parta">Part A: The Power of Diffusion Models!</a>
                    <ul>
                        <li><a href="#parta0">Part 0: Setup</a></li>
                        <li><a href="#parta1">Part 1: Sampling Loops</a></li>
                        <li><a href="#parta11">Part 1.1: Forward Process</a></li>
                        <li><a href="#parta12">Part 1.2: Classical Denoising</a></li>
                        <li><a href="#parta13">Part 1.3: One-Step Denoising</a></li>
                        <li><a href="#parta14">Part 1.4: Iterative Denoising</a></li>
                        <li><a href="#parta15">Part 1.5: Diffusion Sampling</a></li>
                        <li><a href="#parta16">Part 1.6: CFG</a></li>
                        <li><a href="#parta17">Part 1.7: Image-to-Image</a></li>
                        <li><a href="#parta171">Part 1.7.1: Hand-Drawn Images</a></li>
                        <li><a href="#parta172">Part 1.7.2: Inpainting</a></li>
                        <li><a href="#parta173">Part 1.7.3: Text-Conditional</a></li>
                        <li><a href="#parta18">Part 1.8: Visual Anagrams</a></li>
                        <li><a href="#parta19">Part 1.9: Hybrid Images</a></li>
                    </ul>
                </li>
                <li><a href="#partb">Part B: Flow Matching From Scratch!</a>
                    <ul>
                        <li><a href="#partb1">Part 1: Training a Single-Step Denoising UNet</a></li>
                        <li><a href="#partb11">Part 1.1: Implementing the UNet</a></li>
                        <li><a href="#partb12">Part 1.2: Using the UNet to Train a Denoiser</a></li>
                            <ul>
                                <li><a href="b121">Part 1.2.1: Training</a></li>
                                <li><a href="b122">Part 1.2.2: Out-of-Distribution Testing</a></li>
                                <li><a href="b123">Part 1.2.3: Denoising Pure Noise</a></li>
                            </ul>
                        <li><a href="#partb2">Part 2: Training a Flow Matching Model</a></li>
                        <li><a href="#partb21">Part 2.1: Adding Time Conditioning to UNet</a></li>
                        <li><a href="#partb22">Part 2.2: Training the UNet</a></li>
                        <li><a href="#partb23">Part 2.3: Sampling from the UNet</a></li>
                        <li><a href="#partb24">Part 2.4: Adding Class-Conditioning to UNet </a></li>
                        <li><a href="#partb25">Part 2.5: Training the Unet</a></li>
                        <li><a href="#partb26">Part 2.6: Sampling form the UNet</a></li>
                    </ul>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <div class="main-wrapper">
        <div class="container">
            <header class="header">
                <h1>Project 5: Fun with Diffusion Models!</h1>
                <h4>CS 180 Fall 2025 – Maria Rufova</h4>
            </header>

        <!-- Introduction -->
        <section class="introduction" id="introduction">
            <h2>Introduction</h2>
            <p>
                In this project I implemented and deployed diffusion models for image generation.  
                In part A, I worked with the DeepFloyd IF model.
                Diffusion models work by gradually adding noise to images and then learning to reverse this process, enabling them to 
                generate high-quality images from pure noise or perform various image editing tasks. In this part I implemented 
                the core sampling loops and experimented with Classifier-Free Guidance (CFG) to docreative tasks such as
                inpainting, visual anagrams, and hybrid images.
                In part B I will implement Flow Matching from scratch. 
            </p>
        </section>

        <!-- Part 0 -->
        <section class="subsection" id="parta">
            <h1>Part A: The Power of Diffusion Models</h1>
        </section>

        <section class="partA1" id="parta0">
            <h2>Part 0: Setup</h2>
            <p>
                Throughout part A I used the DeepFloyd IF diffusion model, a two-stage text-to-image model that produces 64×64 images. 
                To use the model, I generated prompt embeddings using the provided Hugging Face clusters, creating a dictionary of embeddings for various 
                creative text prompts. Some example prompts and their corresponding images are shown below. 
            </p>
            <p>
                I experimented with different numbers of <code>num_inference_steps</code> (20, 40, 60, 80, 100) to observe how this parameter affects image quality. 
                More inference steps generally produce higher quality images but take longer to generate.
                In the examples below, 100 inference steps produced final images that produced finer details and more closely replicated the prompted art style than
                when those same prompts ran with 20 steps. 
                For all subsequent parts of this project, 
                I used a random seed of <strong>100</strong> to make sure I can reproduce my results later, and kept this same seen throughout the rest of part A. 
            </p>
            <p>
                Three examples of images generated with my custom prompts (20 vs 100 inference steps):
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/ada_100.jpeg" alt="Ada Lovelace" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 20) 'A painting of Ada Lovelace in Alphonse Mucha's style'</figcaption>
                </figure>   
                <figure>
                    <img src="web_images/fuji_100.jpeg" alt="Mount Fuji" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 20) 'A painting of Mount Fuji in Claude Monet's style'</figcaption>
                </figure> 
                <figure>
                    <img src="web_images/ferrari_1.jpeg" alt="Ferrari" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 20) 'A painitng of red Ferrari car in Kazimir Malevich style'</figcaption>
                </figure>      
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/ada_love_2.jpeg" alt="Ada Lovelace" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 100) 'A painting of Ada Lovelace in Alphonse Mucha's style'</figcaption>
                </figure>   
                <figure>
                    <img src="web_images/fuji_3.jpeg" alt="Mount Fuji" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 100) 'A painting of Mount Fuji in Claude Monet's style'</figcaption>
                </figure> 
                <figure>
                    <img src="web_images/ferrari_100.jpeg" alt="Ferrari" style="max-width: 300px; height: auto;">
                    <figcaption>(num_inference_steps = 100) 'A painitng of red Ferrari car in Kazimir Malevich style'</figcaption>
                </figure>      
            </div>
        </section>

        <section class="partA2" id="parta1">
            <h2>Part 1: Sampling Loops</h2>
            <p>
                In this section I wrote my own "sampling loops" that use pretrained DeepFloyd denoisers to produce high quality images. 
                
                A diffusion model works in two phases; first, the forward process 
                gradually adds Gaussian noise to a clean image over T timesteps (T=1000 here), while the reverse process learns 
                to denoise and recover the original image. By starting from pure noise and iteratively denoising, we can generate new images 
                from scratch using what we learned from traiing. 
            </p>
        </section>

        <section class="partA3" id="parta11">
            <h2>Part 1.1: Implementing the Forward Process</h2>
            <p>
                The forward process is defined by: <strong>x_t = √(ᾱ_t) · x_0 + √(1 - ᾱ_t) · ε</strong>, where x_0 is the clean image, 
                x_t is the noisy image at timestep t, and ε is Gaussian noise. The noise schedule coefficients ᾱ_t control how much noise 
                is added at each step.
            </p>
            <p>
                I implemented a function <code>noisy_im = forward(im, t)</code> that takes a clean image <code>im</code> and adds 
                a specific amount of Gaussian noise based on the timestep <code>t</code>. 
                At every timestep t, we get the corresponding noise schedule coefficients ᾱ_t from pre-defined (by people who trained DeepFloyd)
                scheduler parameters <code>alphas_cumprod</code>, sample random noise epsion from N(0, 1) with the same shape as im, and apply the forward
                process formula.
                The formula combines the scaled original image with scaled 
                random noise: <strong>x_t = √(ᾱ_t) · x_0 + √(1 - ᾱ_t) · ε</strong>.
            </p>
            <p>
                I applied this process to 
                the Campanile test image at three different noise levels: t=250 (little noise), t=500 (moderate noise), and t=750 
                (heavy noise). As expected, the image becomes progressively more corrupted as t increases.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_1.jpeg" alt="Forward process on Campanile">
                    <figcaption>The Campanile at Different Noise Levels (t=0, 250, 500, 750)</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA4" id="parta12">
            <h2>Part 1.2: Classical Denoising</h2>
            <p>
                First I tried to denoise my images using classical methods, like here I take my noisy images for steps [250, 500, 750] and 
                use <b>Gaussian blur filtering</b> to try to remove the noise.
                This method simply applies spatial smoothing to reduce noise, without any understanding of the underlying image structure, 
                so the results are not very good at all!
                I experimented with different kernel sizes (3×3 for t=250, 5×5 for t=500, 7×7 for t=750) to find the best denoising 
                for each noise level, but even that adjustement did not save me from a blurry and messy result.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_2.jpeg" alt="Classical denoising comparison">
                </figure>
            </div>
        </section>

        <section class="partA5" id="parta13">
            <h2>Part 1.3: One-Step Denoising</h2>
            <p>
                Now I used the pretrained DeepFloyd UNet to denoise images in a single step. The UNet was trained on millions of 
                noisy/clean image pairs and learned to predict the noise component in a noisy image. So here, given a noisy image x_t and timestep t, 
                the UNet outputs an estimate of the noise ε, which I use to recover the clean image: 
                <strong>x_0 = (x_t - √(1 - ᾱ_t) · ε) / √(ᾱ_t)</strong>.
            </p>
            <p>
                The UNet is conditioned on text prompts (used "a high quality photo" for now). I think that compared to Gaussian blur, one-step denoising produces 
                much cleaner results, successfully recovering recognizable Campanile structure even from the most noisy stages. 
                However, there's a huge trade-off in quality of the image at higher values of t because removing that much noise in one step also removes 
                too many fine details from the underlying original image of the Campanile.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_3.jpeg" alt="One-step denoising results">
                </figure>
            </div>
        </section>

        <section class="partA6" id="parta14">
            <h2>Part 1.4: Iterative Denoising</h2>
            <p>
                Instead of denoising in one-step, I now tried to denoise iteratively (the way diffusion models were designed to do).
                Iterative denoising gradually removes noise over multiple steps. Instead of running all 1000 timesteps (oops, too expensive!), 
                I used a strided schedule (990, 960, 930, ..., 30, 0) with steps of 30 for efficiency.
            </p>
            <p>
                Here I implement the function <code>iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps)</code>.
                At each step, the function (1) predicts x_0 using the UNet, (2) computes the next slightly-less-noisy image x_t' using the DDPM 
                formula that interpolates between the predicted clean image and directional noise, and (3) adds a small amount of 
                random noise (via function <code>add_variance</code>) to keep the model from establishing a strong bias during training. 
                The formula I followed here is <strong>x_t' = √(ᾱ_t') · β_t / (1 - ᾱ_t) · x_0 + √(α_t) · (1 - ᾱ_t') / (1 - ᾱ_t) · x_t + v_\(\sigma \)</strong>.
            </p>
            <p>
                Below I show the denoising progression at the same timesteps as specified in the project spec. As we move back from later (t=690) to 
                earlier timesteps (t=90), the image gradually becomes clearer, demonstrating how iterative refinement produces much 
                better results than one-step denoising. The final comparison 
                shows iterative denoising produces most detailed reconstruction compared to one-step denoising and the Gaussian blur.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_4_a.jpeg" alt="Iterative denoising progression">
                    <figcaption>Denoising progression at t=90, 240, 390, 540, 690 (less noise → more noise)</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_4_b.jpeg" alt="Denoising methods comparison">
                    <figcaption>Comparison: Original, Iteratively Denoised, One-Step Denoised, Gaussian Blur</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA7" id="parta15">
            <h2>Part 1.5: Diffusion Model Sampling</h2>
            <p>
                Now that we have iterative denoising working, I can now generate images from scratch. I do this by generating pure random noise (x_T) 
                and running iterative denoising from i_start=0, which leads to the model hallucinating entirely new image that match the prompt (still "a high quality photo" here).
            </p>
            <p>
                Here are 5 sample images I got with that prompt. The model produces diverse samples with familiar-looking scenes and characters, 
                but there is a clear lack of quality and details. Now we need to figure out a way to create a stronger link between the text prompt and the 
                iamge generation process in order to produce better results. 
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_5_b.jpeg" alt="Diffusion model samples">
                    <figcaption>5 images sampled from pure noise with prompt "a high quality photo"</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA8" id="parta16">
            <h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>
            <p>
                In order to improve image quality (at the expense of image diversity), I implemented Classifier-Free Guidance to amplify the influence of the text prompt. 
                This approach computes both conditional (ε_cond with prompt) and unconditional (ε_uncond with empty prompt "") noise estimates, and then 
                extrapolates away from the unconditional estimate for our new noise estimate: <strong>ε = ε_uncond + γ · (ε_cond - ε_uncond)</strong>.
            </p>
            <p>
                The γ in this formual is the scale that controls the sterngth of CFG (for γ = 0 we get an unconditional noise estimate, and for γ = 1
                we get the conditional noise estimate). With γ=7, the model produces much sharper, more coherent images that strongly adhere to the prompt. The tradeoff is 
                reduced diversity, because higher CFG values sacrifice variety for quality. 
            </p>
            <p>
                In <code>iterative_denoise_cfg</code> function, for each denoising step, I pass the current noisy image through the UNet twice. Once with <code>prompt_embeds</code> to get
                the conditional noise estimate ε_cond, and once with <code>uncond_prompt_embeds</code> (empty string "") to get the 
                unconditional noise estimate ε_uncond. I then use the formula above to compute the guided noise estimate, where with our scale of 7, 
                this extrapolates away from the unconditional prediction in the direction of the conditional prediction,  amplifying the prompt's influence. 
                The resulting images are much more detailed and produce more realistic looking people, places, and objects.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_6_b.jpeg" alt="CFG samples">
                    <figcaption>5 images with CFG (scale=7)</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA9" id="parta17">
            <h2>Part 1.7: Image-to-Image Translation</h2>
            <p>
                Using the SDEdit algorithm, I edit existing images by controlling how much noise is added before denoising.
                 For each i_start value in [1, 3, 5, 7, 10, 20], I use my <code>forward</code> function to add noise at timestep <code>t = strided_timesteps[i_start]</code>, 
                 then denoise with <code>iterative_denoise_cfg</code> using scale=7. Lower i_start values add minimal noise (preserving the original structure), 
                 while higher values add more noise (allowing greater creative freedom). 
                
                I tested this on the original image of the Campanile, as well as two of my own photos of a toy horse and my friend Basil.
            </p>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_7_a.jpeg" alt="Campanile SDEdit">
                    <figcaption>Campanile edits at noise levels 1, 3, 5, 7, 10, 20 (and original)</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_7_b.jpeg" alt="Horse SDEdit">
                    <figcaption>Toy horse edits at noise levels 1, 3, 5, 7, 10, 20 (and original)</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_b.jpeg" alt="Basil SDEdit">
                    <figcaption>A photo of my friend Basil edits at noise levels 1, 3, 5, 7, 10, 20 (and original)</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA10" id="parta171">
            <h2>Part 1.7.1: Editing Hand-Drawn and Web Images</h2>
            <p>
                The SDEdit technique works exceptionally well on non-realistic inputs. 
                I applied the same SDEdit pipeline from Part 1.7 to three sketches: a sketch of Berkeley I found on the web 
                 and my hand-drawn sheep and tomato. 
                 The diffusion model projects these simple line drawings onto the natural image manifold, adding realistic textures, lighting, and details while preserving the original sketch composition.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/berk_sketch.jpeg" alt="Berkeley sketch" style="max-width: 200px; height: auto;">
                </figure>
                <figure>
                    <img src="web_images/sheep_sketch.jpeg" alt="Sheep sketch">
                    <figcaption>Original drawings</figcaption>
                </figure>
                <figure>
                    <img src="web_images/tomato_sketch.jpeg" alt="Tomato sketch">
                </figure>
            </div>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_1_web.jpeg" alt="Berkeley processed">
                    <figcaption>Berkeley sketch transformed at different noise levels</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/sheep_process.jpeg" alt="Sheep processed">
                    <figcaption>Sheep sketch transformed at different noise levels</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/tomato_progress.jpeg" alt="Tomato processed">
                    <figcaption>Tomato sketch transformed at different noise levels</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA11" id="parta172">
            <h2>Part 1.7.2: Inpainting</h2>
            <p>
                I used the same procedure to implement inpainting, following the RePaint paper.
                 I implemented inpainting to fill masked regions with new content. The algorithm runs the 
                normal denoising loop, but at each step, replaces the unmasked regions with the properly-noised original image.
            </p>
            <p>
                Following the RePaint paper, my <code>inpaint</code> function modifies the standard denoising loop by
                creating a binary mask where 1 indicates regions to regenerate and 0 indicates regions to preserve. 
                At each denoising step, after computing <code>pred_prev_image</code> with CFG, I replace the unmasked regions using <strong>pred_prev_image = mask · pred_prev_image + (1 - mask) · forward(original_image, prev_t)</strong>. 
                The resulting image preserves the original photo elements from outside the mask while generating new creative content inside it.
                Below is an example of inpainted Campanile (its clock got replaced with lighthouse top).
                I also applied it to my photos of a toy horse (which now has a rider) and my friend (who got a pink hat).
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/part_1_7_1_c.jpeg" alt="Campanile inpaint">
                    <figcaption>Campanile: Original, Mask, Region to Replace, Inpainted Result</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_2_horse.jpeg" alt="Horse inpaint">
                    <figcaption>Horse inpainting result</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_2_basil.jpeg" alt="Basil inpaint">
                    <figcaption>Basil inpainting result</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA12" id="parta173">
            <h2>Part 1.7.3: Text-Conditional Image-to-Image Translation</h2>
            <p>
                Building on SDEdit, I now guide the image projection with specific text prompts instead of the generic "a high quality photo". 
                This provides semantic control over the editing process, and I can make the model generate a new image that follows the prompt while
                preserving the original image composition.
            </p>
            <p>
                Below are my experiments with prompts like "a marble statue", "a rocket ship", "sourdough bread dough rising", and "a colorful garden 
                with flowers". At low noise levels (i_start=1), the transformations are subtle, but at higher noise levels (i_start=20), 
                the subjects take on clear characteristics of the prompt.
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_rocket.jpeg" alt="Rocket Campanile">
                    <figcaption>'A rocket ship' transformation of Campanile</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_a.jpeg" alt="Marble Campanile">
                    <figcaption>'A marble statue' transformation of Campanile</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_e.jpeg" alt="Bread horse">
                    <figcaption>'Sourdough bread dough rising' transformation of horse</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_7_3_c.jpeg" alt="Garden Basil">
                    <figcaption>'A colorful garden with flowers' transformation of Basil</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA13" id="parta18">
            <h2>Part 1.8: Visual Anagrams</h2>
            <p>
                Visual anagrams are optical illusions where an image appears as one thing normally but reveals a different picture when flipped upside down. 
                I implemented this in my <code>visual_anagrams</code> function by running the UNet four times per denoising step: 
                twice for the normal orientation (conditional and unconditional with prompt_embeds_1) and twice for the flipped orientation 
                (conditional and unconditional with prompt_embeds_2). I flip the image upside down using <code>torch.flip(image, dims=[2])</code>, 
                compute CFG noise estimates for both orientations, flip the second noise estimate back, and average them. 
                This averaged noise is then used in the standard DDPM denoising formula I used before. 
            </p>
            <p>
                Below are some examples of the anagrams I created (titles are the conditioning prompts I used):
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_8_drac.jpeg" alt="Dracula anagram">
                    <figcaption>Count Dracula / Gothic Cathedral visual anagram</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_8_man_b.jpeg" alt="Old man anagram">
                    <figcaption>Oil painting of old man / Natural landscape visual anagram</figcaption>
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="web_images/1_8_piano_b.jpg" alt="Piano view" style="max-width: 500px; height: auto;">
                    <figcaption>A grand piano viewed from above</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_8_stairs_b.jpg" alt="Stairs view" style="max-width: 500px; height: auto;">
                    <figcaption>A staircase going up</figcaption>
                </figure>
            </div>
        </section>

        <section class="partA14" id="parta19">
            <h2>Part 1.9: Hybrid Images</h2>
            <p>
                We can also use this diffusion model to create hybrid images that show different content at different viewing distances. 
                My <code>make_hybrids</code> function computes CFG noise estimates for two different prompts, then separates high and low frequencies.
                I extract low frequencies from the first noise estimate using <code>TF.gaussian_blur</code>, and compute high frequencies from the second estimate using 
                <strong>high_freqs = cfg_noise_est_2 - gaussian_blur(cfg_noise_est_2)</strong>. 
                The combined noise estimate <strong>ε</strong> is then used for denoising. 
                Here are some hybrids I got:
            </p>

            <div class="gallery">
                <figure>
                    <img src="web_images/1_9_skull.jpg" alt="Skull hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid of Skull and Waterfall </figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_9_birds_c.jpeg" alt="Birds hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid of sheet music and  birds on a wire</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_9_palette.jpg" alt="Palette hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid of a colorful garden and an artist's palette</figcaption>
                </figure>
                <figure>
                    <img src="web_images/1_9_rose.jpeg" alt="Rose hybrid" style="max-width: 400px; height: auto;">
                    <figcaption>Hybrid of a swirled frosting and a rose flower</figcaption>
                </figure>
            </div>
        </section>

        <section class="subsection" id="partb">
            <h1>Part B: Flow Matching From Scratch!</h1>
            <p>In Part B I implemented a flow matching model from scratch to generate MNIST digits. 
                I did so by building and training a UNet architecture to learn the mapping from pure noise to clean images. 
                It uses flow matching to learn a continuous flow vector field  that transforms random Gaussian noise into 
                data samples through iterative denoising.

            </p>
        </section>

        <section class="partB" id="partb1">
            <h2>Part 1: Training a Single-Step Denoising UNet</h2>
            <p>
                I started by buildng a single step UNet denoiser, where the trained model takes a noisy MNIST image and directly predicts 
                the clean version in one single forward pass.
                The training objective here is to minimize the L2 loss between the model's prediction and the actual clean image.
            </p>
        </section>

        <section class="partB" id="partb11">
            <h2>Part 1.1: Implementing the UNet</h2>
            <p>
                My implemented UNet architecture follows the diagram shown below, with multiple upsampling and downsampling blocks and skip connections.
                The UNet (called this because of its "U" shape) consists of an encoder part that progressively downsamples the input, 
                a bottleneck that compresses the downsampled features to a 1x1 spacial dimension, 
                and a final decoder part that upsamples it back to the original resolution.
                The architecture also includes important skip connections that concatenate features from the encoder to the decoder at the 
                matching resolutions, allowing the network to preserve fine spacial details that got lost during the downsampling.
                The downsampling and upsampling operations also include batch normalization and GELU activation for stable training.
            </p>
            <p>
                See the full UNet architecture and its individual building blocks in the diagram below:
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/unconditional_unet.jpeg" alt="unet" >
                    <figcaption> Unconditional UNet </figcaption>
                </figure>
                <figure>
                    <img src="images_5b/unet_operations.jpeg" alt="unet">
                    <figcaption>Standard UNet Operations</figcaption>
                </figure>
            </div>
        </section>

        <section class="partB" id="partb12">
            <h2>Part 1.2: Using the UNet to Train a Denoiser</h2>
            <p>
                After imlpementing the UNet, I need to train it. To create noisy training data, I applied Gaussian noise at different levels 
                (σ = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]) to clean MNIST images using the formula z = x + σε where x is a clean MNIST digit, 
                σ is the noise level, ε ~ N(0, I), and z is the resulting noisy image. Visualization below shows how the increasing noise level 
                corrupts the original image more and more. We're going to neet a lot of these training pairs (noisy, clean) to train out UNet with later
                on how to remove noise.

            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/1_2_noisy.jpeg" alt="unet" >
                </figure>
            </div>
        </section>

        <section class="partB" id="partb121">
            <h2>Part 1.2.1: Training</h2>
            <p>
                I trained the UNet denoiser on MNIST with a fixed noise level of σ=0.5. 
                For each training batch, I added new random Gaussian noise to a clean image. 
                The model was trained with batch size 256, learning rate 1e-4, and hidden dimension D=128 for 5 epochs using the 
                Adam optimizer. 
            </p>
            <p>
                The training loss curve below shows steady convergence, with the MSE loss decreasing smoothly as the model 
                learns to predict clean images from noisy inputs.
                The chart below also shows denoising results after 1 and after 5 epochs, with epoch 5 outputs being much cleaner
                then epoch 1 outputs.
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/121_chart.jpeg" alt="unet" >
                    <figcaption> Result after training for 1 and for 5 epochs </figcaption>  
                </figure>
                <figure>
                    <img src="images_5b/121_curve.jpeg" alt="unet">
                    <figcaption>Training loss curve for training process of σ=0.5</figcaption>
                </figure>
            </div>
        </section>

        <section class="partB" id="partb122">
            <h2>Part 1.2.2: Out-of-Distribution Testing</h2>
            <p>
                Although our denoiser was trained exclusively on σ=0.5 noise, I also tested it on different σ 's that it wasn't designed for.
                The denoiser performs best at σ=0.5 (its training level), producing clean and accurate reconstructions, as well as on all 
                lower σ 's where the smaller amount of noise still makes the digit easily recognizeable.

                But for σ > 5, the model begins to struggle due to higher amount of noise and thus produces poorer results, with the predicted
                digits looking more crooked.
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/122_testing.jpeg" alt="unet" >
                    <figcaption> (Top: Input, Middle: Noisy with specified σ applied, Bottom: Output)  </figcaption>
                </figure>

            </div>
        </section>

        <section class="partB" id="partb123">
            <h2>Part 1.2.3: Denoising Pure Noise</h2>
            <p>
                To make denoising a generative task, we need to denoise pure, random Gaussian noise (σ=1.0).
                With MSE loss, the optimal solution is to predict 
                the mean (centroid) of all training examples, because this minimizes the expected squared distance to any possible target.
                The results seen below look very blurry and generic (is that an 8? a 9? a 3?). The generated output shows a pattern of
                very blurry, averaged digit-like blobs rather than crisp individual digits.
                All outputs look similar regardless of the input noise because the model learns to output an "average MNIST digit" rather than a 
                specific number.
                This happens because the model has no way to determine which specific digit to 
                generate from pure noise, so it tries to guess and output something that minimally distances itself from all 
                possibilities. Without iterative refinement or additional 
                conditioning information, the model cannot produce accurate high-quality samples.

            </p>

            <div class="gallery">
                <figure>
                    <img src="images_5b/123_chart.jpeg" alt="unet" >
                    <figcaption> Results after denoising pure noise for 1 and 5 epochs </figcaption>  
                </figure>
                <figure>
                    <img src="images_5b/123_curve.jpeg" alt="unet">
                    <figcaption>Training loss curve for pure noise denoising</figcaption>
                </figure>
            </div>
        </section>

        <section class="partB" id="partb2">
            <h2>Part 2: Training a Flow Matching Model</h2>
            <p>
                Turns out denoising everything in one step is not that effective. Oops!
                So we improve our UNet by imlementing <b>Flow Matching!</b> 
                Flow Matching learns a continuous flow field that describes how to move from pure noise (at t=0) to clean data (at t=1).
                The model learns to predict the velocity v_t = x_1 - x_0 at any interpolated point x_t = (1-t)x_0 + t*x_1. 
                By following this learned flow with small steps, we can generate high-quality samples through iterative denoising. 
            </p>
        </section>

        <section class="partB" id="partb21">
            <h2>Part 2.1: Adding Time Conditioning to UNet</h2>
            <p>
                I injected scalar <i>t</i> into the UNet model following the architecture below. 
                For flow matching to work, the UNet must know where it is along the denoising trajectory (t ∈ [0,1]). 
                I extended the unconditional UNet by adding time conditioning through FCBlocks (Fully Connected Blocks). 
                The scalar timestep t is processed through two separate FCBlocks to create embeddings that inject the conditioning 
                signal into the UNet's internal features at two key locations (after the bottleneck and after the first upsampling block).
            </p>
            <p>
                This addition allows the network to adapt its behavior based on the noise level. 
                At early timesteps (t≈0), 
                the input is about pure noise and the model focuses on coarse structure, and at later timesteps (t≈1), 
                the input is almost clean and the model refines fine details.
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/conditioned_unet.jpeg" alt="unet" >
                    <figcaption> Conditioned UNet </figcaption>  
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/fcblock.jpeg" alt="unet">
                    <figcaption>FCBlock for conditioning</figcaption>
                </figure>
            </div>
        </section>

        <section class="partB" id="partb22">
            <h2>Part 2.2: Training the UNet</h2>
            <p>
                I then needed to train the new UNet to predict the flow v_t = x_1 - x_0 at randomly sampled timesteps.
                During training, for each clean image x_1, I sampled pure Gaussian noise x_0 and a random timestep t. 
                I then computed the interpolation x_t = (1-t)x_0 + t*x_1, and trained the UNet to predict the true flow. 
                I used a smaller network (D=64) and larger learning rate (1e-2) compared 
                to Part 1, with an exponential learning rate scheduler (γ=0.9996^(1/0.1)) that gradually decays the learning rate over 
                20 epochs.
            </p>
            <p>
                The training loss curve plot below shows smooth convergence as the model learns the flow field and learns a continous mapping
                across all noise levels.
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/22_curve.jpeg" alt="unet" >
                    <figcaption> Training loss curve for time-conditioned UNet over 20 epochs </figcaption>  
                </figure>
            </div>
        </section>

        <section class="partB" id="partb23">
            <h2>Part 2.3: Sampling from the UNet</h2>
            <p>
                Now that we have a new and improved trained UNet, below are some iterative sampling results for generating MNIST digits
                from pure noise. 
                Starting with z_0 ~ N(0, I), I follow the learned flow field by repeatedly predicting the velocity v_t and taking small 
                Euler steps: z_{t+Δt} = z_t + v_t · Δt. Over 50 timesteps, this gradually transforms random noise into recognizable digits.
            </p>
            <p>
                Some results are still quite messy, but you can see noticeable improvement across epochs. After epoch 1, the digits are very wobbly 
                and blurry, looking more like scribbles rather than numbers. But by Epoch 5, we can notice them taking more distinctive and clear shapes, 
                with most looking very distinct by Epoch 10.
                But one big issue right now is that we cannot control which digit is generated, as the model currently produces random digits 0-9 based on the 
                random initial noise. Enter: Class Conditioning!
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/23_sampling.jpeg" alt="unet" >
                    <figcaption> Top Row: Epoch 1, Middle Row: Epoch 5, Bottom Row: Epoch 10 </figcaption>  
                </figure>
            </div>
        </section>

        <section class="partB" id="partb24">
            <h2>Part 2.4: Adding Class-Conditioning to UNet</h2>
            <p>
                Class-conditioning in UNet means injecting specific class label (like "apple", "cat", or "MNIST digit 3") into the flow
                so that the UNet learns to generate images that are specifically conditioned on that category.
                In class-conditioning, the model accepts both a timestep t and a class label c (one-hot encoded for digits 0-9).
                I added two more FCBlocks to process the class embedding, bringing the total to four conditioning blocks (two for time (fc1_t, fc2_t) and two for class (fc1_c, fc2_c)).
                This design combines class and time information (unflatten = c₁ * unflatten + t₁ and up1 = c₂ * up1 + t₂). 
                This allows the network to use class information to control content (which digit to generate) while using time information 
                to control the noise level. 
                I implemented classifier-free guidance (CFG) training by randomly dropping the class conditioning 10% of the time, 
                which at sampling time allows us to compute both conditional and unconditional flows and extrapolate (v_guided = v_uncond + γ(v_cond - v_uncond))
                , amplifying the class signal for higher quality generation.
            </p>
        </section>

        <section class="partB" id="partb25">
            <h2>Part 2.5: Training the UNet</h2>
            <p>
                Training the class-conditioned UNet is very similar to the flow matching from before, but now with class labels included. 
                During training, for each clean image x_1 and its label, I convert the label to a one-hot vector, apply 10% class dropout for  CFG, 
                sample noise and timestep, and train the UNet to predict the flow. The curve below shows training loss decreasing smoothly, meaning the model
                gradually learns to incorporate class info into the flow prediction.
            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/25_curve.jpeg" alt="unet" >
                    <figcaption> Training loss curve plot for the class-conditioned UNet </figcaption>  
                </figure>
            </div>
        </section>

        <section class="partB" id="partb26">
            <h2>Part 2.6: Sampling from the UNet</h2>
            <p>
                Now we will sample with class-conditioning and will use classifier-free guidance with γ=5.0.
                For each digit 0-9, I generate 4 instances by creating a batch of labels, running the iterative sampling process with CFG scale γ=5.0
                The results below show much better improvement across epochs! 
                After Epoch 1 the digits are already visible thanks to conditioning, by Epoch 5 they get cleaner, and by Epoch 10 they are very clearly defined and 
                recognized 0-9 digits that we were expecting.

            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/26_epoch_1.jpeg" alt="unet" >
                    <figcaption> Epoch 1 </figcaption>  
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/26_epoch_5.jpeg" alt="unet" >
                    <figcaption> Epoch 5 </figcaption> 
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/26_epoch_10.jpeg" alt="unet" >
                    <figcaption> Epoch 10 </figcaption> 
                </figure>
            </div>
        </section>

        <section class="partB" id="partb26">
            <h2>Part 2.6: Sampling from the UNet - Without Learning Rate Scheduler</h2>
            <p>
                To simplify the training process, I tried removing the annoying exponential learning rate scheduler. 
                I retrained the class-conditioned model from scratch using a constant learning rate of 5e-3 (half the original 1e-2) 
                without any scheduler. This lower constant learning rate prevents overshooting and maintains stability throughout training.
            </p>
            <p>
                As seen below, the results from No Scheduler experiment look nearly identical to when I was using the Scheduler.
                The training loss curve and final sampled digits are very similar.
                The class conditioning signal is strong enough that the model converges well  even with a fixed learning rate. 
                This shows that when the conditioning signal is very informative (as with class labels),
                the model learns well without needing more complicated learning rate schedules.

            </p>
            <div class="gallery">
                <figure>
                    <img src="images_5b/26_comparison.jpeg" alt="unet" >
                    <figcaption> Top: Scheduler , Bottom: No Scheduler </figcaption>  
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/nosched_curve.jpeg" alt="unet" >
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/nosched_epoch_1.jpeg" alt="unet" >
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/nosched_epoch_5.jpeg" alt="unet" >
                </figure>
            </div>
            <div class="gallery">
                <figure>
                    <img src="images_5b/nosched_epoch_10.jpeg" alt="unet" >
                </figure>
            </div>
        </section>
    </div>
    <script>
        window.addEventListener('scroll', function() {
            let sections = document.querySelectorAll('section[id]');
            let scrollPosition = window.scrollY + 100;
            sections.forEach(section => {
                let sectionTop = section.offsetTop;
                let sectionHeight = section.offsetHeight;
                let sectionId = section.getAttribute('id');
                
                if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
                    document.querySelectorAll('.sidebar a').forEach(link => {
                        link.classList.remove('active');
                    });
                    let activeLink = document.querySelector(`.sidebar a[href="#${sectionId}"]`);
                    if (activeLink) {
                        activeLink.classList.add('active');
                    }
                }
            });
        });
    </script>
</body>
</html>